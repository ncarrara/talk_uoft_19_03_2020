\documentclass{beamer}
\usetheme[secheader]{Boadilla}
\usepackage{comment}
% wow this is a hack that lets you have more definitions in latex.
% see http://www.tex.ac.uk/cgi-bin/texfaq2html?label=noroom
\usepackage{natbib}
\usepackage{etex}
\usepackage{pgffor}
\usepackage[utf8]{inputenc}
%\usepackage[cyr]{aeguill}
\reserveinserts{28}
\usepackage{tikz}
\usepackage[customcolors]{hf-tikz}
%\usepackage[utf8]{inputenc}
%\mode<presentation>{\usetheme{Caltech}}

\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{multimedia}
\usepackage{color}
\usepackage{esint}
\usepackage{stmaryrd}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage[squaren]{SIunits}
\usepackage{graphicx}
\usepackage{diagbox}
\usepackage{pdfpages}
\usepackage{dsfont}
\usepackage{xcolor}
\usepackage{soul}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

\usepackage{subfig}

\graphicspath{{./img/}}

\newcommand{\cplus}{\colorbox{green}{($+$)} }
\newcommand{\cmoins}{\colorbox{red}{($-$)} }
\newcommand{\cmean}{\colorbox{yellow}{($\pm$)}}

\newcommand{\mathcolorbox}[2]{\colorbox{#1}{$\displaystyle #2$}}

\input{mathdef.tex}
%\input{../glossary.tex}
\input{symbol.tex}

\setbeamertemplate{footline}{%
    \hfill%
    \usebeamercolor[fg]{page number in head/foot}%
    \usebeamerfont{page number in head/foot}%
    \insertframenumber%
%\,/\,\inserttotalframenumber
    \kern1em\vskip2pt%
}

\beamertemplatenavigationsymbolsempty

\author[shortname]{
    Nicolas Carrara \inst{3} \and
    Edouard Leurent\inst{3,5} \and
    Tanguy Urvoy \inst{1} \and
    Romain Laroche \inst{2} \and
    Odalric-Ambrym Maillard \inst{3} \and
    Olivier Pietquin \inst{3,4}}
\institute[shortinst]{\inst{1} Orange Labs\and %
    \inst{2} Microsoft Montr\'eal. \and
    \inst{3} Univ. Lille, CNRS, Centrale Lille, INRIA UMR 9189 - CRIStAL\and
    \inst{4} Google Research, Brain Team, Paris\and
    \inst{5} Renault}

\title[]{Budgeted Reinforcement Learning in Continuous State Space (NeurIPS 2019)}
\date{Last update: August the 11th, 2019.}
\begin{document}
    \begin{frame}
        \maketitle
        \centering
    \end{frame}


    \foreach \n in {0,1}{
        \begin{frame}{The Transfer Learning problem}
            \begin{figure}
                \centering
                %%\vspace{-1.5em}
                \includegraphics[scale=0.75,page=1]{tl\n.pdf}
            \end{figure}
        \end{frame}
    }

    \begin{frame}{Why we transfer?}

        \begin{figure}
            \begin{center}
                \subfloat[Jumpstart]{
                    \includegraphics[width=0.3\textwidth]{objectives-jumpstart}
                    \label{fig:objectives-jumpstart}
                }
                \subfloat[Learning]{
                    \includegraphics[width=0.3\textwidth]{objectives-learn}
                    \label{fig:objectives-learn}
                }
                \subfloat[Asymptotical]{
                    \includegraphics[width=0.3\textwidth]{objectives-asymptote}
                    \label{fig:objectives-asymptote}
                }
                \caption{Transfer learning objectives ~(Langley 2016)}
                \label{fig:objectives}
            \end{center}
        \end{figure}

    \end{frame}

    \begin{frame}{The settings}

        Two main setting:

        \begin{itemize}
            \item Domain $S\times A$ is not fixed. Find a mapping between a source and a target task.
            \item Domain is fixed:
            \begin{itemize}
                \item mono-task (one to one),
                \item generic-task (one to many),
                \item multi-task  (many to many).
            \end{itemize}
        \end{itemize}

    \end{frame}

    \begin{frame}{The knowledge transfered}

        Several types of knowledge:
        \begin{itemize}
            \item transitions ( ${(s_i,a_i,r'_i,s'_i)}_{i\in[0,N]}$ for example),
            \item representation (features, action space, options, reward shaping etc),
            \item parameters (meta parameters, Q-function, policy etc).
        \end{itemize}

    \end{frame}

    \begin{frame}{Negative Transfer}

        \begin{itemize}
            \item Transfer learning isnâ€™t guaranteed to help
            \item Negative transfer occurs when performance is worse than learning
            target task from scratch (usually happen at the asymptote)
        \end{itemize}

    \end{frame}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


    \begin{frame}
        Adaptation workflow [Carrara et al. 2017, Sigdial]
    \end{frame}


    \foreach \n in {1,2,3,4,5,6,7,8,9,10,11,12}{
        \begin{frame}{\textbf{Adaptation} | processus}
            \begin{figure}
                \begin{center}
                    \includegraphics[width=0.75\textwidth]{dataflowRobot\n.pdf}
                \end{center}
            \end{figure}
        \end{frame}
    }


    \begin{frame}{\textbf{Sources election} | \textsc{PD-distance}}
        \begin{figure}
            \begin{center}
                \includegraphics[width=0.8\textwidth]{pddistance0.pdf}
            \end{center}
        \end{figure}
    \end{frame}

    \begin{frame}{\textbf{Sources election} | \textsc{PD-distance}}
        \begin{figure}
            \begin{center}
                \includegraphics[width=0.8\textwidth]{pddistance.pdf}
            \end{center}
        \end{figure}
    \end{frame}
    \foreach \n in {0,1,2,3,4}{
        \begin{frame}{\textbf{Sources election} | \textsc{PD-distance} and euclidian norm}
            \begin{figure}
                \begin{center}
                    \includegraphics[width=0.8\textwidth]{euclide\n.pdf}
                \end{center}
            \end{figure}
        \end{frame}
    }

    \begin{frame}{\textbf{Sources election} | \textsc{K-means}}
        \begin{figure}
            \begin{center}
                \includegraphics[width=0.8\textwidth]{clustering.pdf}
            \end{center}
        \end{figure}
    \end{frame}

    \foreach \n in {0}{
        \begin{frame}{\textbf{Sources election} | \textsc{K-means}}
            \begin{figure}
                \begin{center}
                    \includegraphics[width=0.8\textwidth]{kmeans\n.pdf}
                \end{center}
            \end{figure}
        \end{frame}
    }
    \begin{frame}{\textbf{Sources election} | \textsc{K-medoids}}
        \begin{figure}
            \begin{center}
                \includegraphics[width=0.8\textwidth]{clustering.pdf}
            \end{center}
        \end{figure}
    \end{frame}

    \begin{frame}{\textbf{Sources election} | \textsc{K-medoids}}
        \begin{figure}
            \begin{center}
                \includegraphics[width=0.8\textwidth]{kmedoids.pdf}
            \end{center}
        \end{figure}
    \end{frame}



    \foreach \n in {8}{
        \begin{frame}{\textbf{Source selection} | Bandit: UCB1}
            \begin{figure}
                \begin{center}
                    \includegraphics[width=0.65\textwidth]{bd\n.pdf}
                \end{center}
            \end{figure}
        \end{frame}
    }


    \foreach \n in {0,1,2,3,4,5,6}{
        \begin{frame}{\textbf{Transfer and Learning}}
            \begin{figure}
                \begin{center}
                    \includegraphics[width=1.0\textwidth]{tftq\n.pdf}
                \end{center}
            \end{figure}
        \end{frame}
    }


    \begin{frame}{\textbf{Adaptation experiments} | simulated users}
        \begin{figure}
            \captionsetup[subfigure]{labelformat=empty}
            \begin{center}
                \subfloat[Overall score ]{
                    \includegraphics[width=0.5\textwidth]{img/handcraftedScores.pdf}
                }
                \subfloat[Dialogue size]{
                    \includegraphics[width=0.5\textwidth]{img/handcraftedDialoguesize.pdf}
                }
            \end{center}
        \end{figure}
    \end{frame}

    \begin{frame}{\textbf{Adaptation experiments} | human-model users}
        \begin{figure}
            \captionsetup[subfigure]{labelformat=empty}
            \begin{center}
                \subfloat[Overall score ]{
                    \includegraphics[width=0.5\textwidth]{img/humanScores.pdf}
                }
                \subfloat[Dialogue size]{
                    \includegraphics[width=0.5\textwidth]{img/humanDialoguesize.pdf}
                }
            \end{center}
        \end{figure}
    \end{frame}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


    \section{Safe Reinforcement Learning}

    \begin{frame}
        Budgeted Reinforcement Learning [Carrara et al. 2019, NeurIPS]
    \end{frame}

    \begin{frame}

        \begin{alertblock}{Limitation}
            New tasks $\rightarrow$ requiere caution sometimes.
        \end{alertblock}


        \begin{exampleblock}{Solution}
            \begin{itemize}
                \item Learning the policy for the target task,
                \item using a safe policy.
                \begin{itemize}
                    \item Budget $\beta$ is the average amount of risk allowed.
                \end{itemize}
            \end{itemize}
        \end{exampleblock}

        \begin{block}{}
            How to learn this conservative policy ?
        \end{block}

    \end{frame}

    \subsection{Mastering the risk in RL}

    \begin{frame}

        \begin{block}{Constraint}
            $C:S\times A \rightarrow \{0,1\}$.
        \end{block}

        \begin{block}{Lagragian relaxation}
            $R \leftarrow R - \lambda C$.
        \end{block}


        \begin{alertblock}{Limitation}
            What $\lambda$ for what budget $\beta$?
        \end{alertblock}

        \begin{exampleblock}{Solution: $\lambda$ calibration}% oral: limitation des politiques lagragiennes
            Searching the safe policy on the Pareto front.
        \end{exampleblock}


    \end{frame}

    \foreach \n in {0,1}{
        \begin{frame}{}
            \begin{figure}
                \begin{center}
                    \includegraphics[width=1\textwidth]{pareto\n.pdf}
                \end{center}
            \end{figure}
        \end{frame}
    }

    \begin{frame}
        \begin{alertblock}{Limitations}
            \begin{itemize}
                \item \cmoins Cumbersome and not reliable processus  % oÃ¹ commencer, quel steps ?
                \item \cmoins It is possible that not $\lambda$ exists for $\beta$.
                \item \cmoins Deterministics policies $\rightarrow$ extreme behaviours.
                % pas de formulation linÃ©aire de la reward pour un budget donnÃ©
            \end{itemize}
        \end{alertblock}
        \begin{exampleblock}{Solution}
            Budgeted Reinforcement Learning.
            % Ce qui nous amÃ¨ne Ã  la prochaine contribution: BRL
        \end{exampleblock}
    \end{frame}


    \begin{frame}{Framework}
        \begin{block}{Markov Decision Processes (MDP)}
            \begin{itemize}
                \item $(\cS, \cA, P, R, \gamma)$
                \item $G_r^\pi = \sum_{t=0}^\infty \gamma^t R(s_t, a_t)$ return of rewards.
                \item Trouver $\pi^*$ t.q $\forall s\in\cS$:
                \begin{equation}
                    \label{eq:mdp}
                    \begin{array}{lcr}
                        \displaystyle \pi^* \in \argmax_{\pi\in\cM(\cA)^\cS} \expectedvalue[G_r^\pi | s_0=s]
                    \end{array}
                \end{equation}

            \end{itemize}
        \end{block}


        \begin{block}{}
            \begin{itemize}
                \item \cplus \textit{Tractable}
                \item \cmoins Extreme behaviours
                \item \cmoins Need calibration (which $\lambda$ for $\beta$?)
                % A l ORAL si formulation sous contrainte
            \end{itemize}
        \end{block}

    \end{frame}

    \begin{frame}{Framework}

        \begin{block}{\textcolor{purple}{Constrained} Markov Decision Processes (CMDP)}
            \begin{itemize}
                \item $(\cS, \cA, P, R,\textcolor{purple}{C}, \gamma,\textcolor{purple}{\beta})$
                \item $G_r^\pi = \sum_{t=0}^\infty \gamma^t R(s_t, a_t)$ return of rewards.
                \item \textcolor{purple}{ $G_c^\pi = \sum_{t=0}^\infty \gamma^t C(s_t, a_t)$ return of costs.}
                \item Trouver $\pi^*$ t.q $\forall s\in\cS$:
                \begin{equation}
                    \label{eq:cmdp}
                    \begin{array}{lcr}
                        \displaystyle \pi^* \in \argmax_{\pi\in\cM(\cA)^\cS} \expectedvalue[G_r^\pi | s_0=s] \\
                        \text{ s.t. }  \textcolor{purple}{\expectedvalue[G_c^\pi | s_0=s] \leq \beta}
                    \end{array}
                \end{equation}
            \end{itemize}
        \end{block}

        \begin{block}{}
            \begin{itemize}
                \item \cplus \textit{Tractable}
                \item \cmean +1 DOF (if policies mixtures) % A L ORAL On peut dÃ©finir un budget de safety , formuation plus naturelle
                \item \cmoins fixed budged
                % A L ORAL si le budget change on the fly, ou qu'il n'est pas adaptÃ©, on doit reapprendre une politique,  Or Le front de pareto est rarement linÃ©aire, le choix du budget n'est pas Ã©vident


            \end{itemize}
        \end{block}

    \end{frame}

    \begin{frame}{Framework}

        \begin{block}{\textcolor{purple}{Budgeted} Markov Decision Process (BMDP)}
            \begin{itemize}
                \item $(\cS, \cA, P, R,{C}, \gamma,\textcolor{purple}{\cB})$
                \item $G_r^\pi = \sum_{t=0}^\infty \gamma^t R(s_t, a_t)$ return of rewards.
                \item  $G_c^\pi = \sum_{t=0}^\infty \gamma^t C(s_t, a_t)$ return of costs.
                \item Find $\pi^*$ s.t $\forall (s,\textcolor{purple}{\beta})\in\cS\times\textcolor{purple}{\cB}$:
                \begin{equation}
                    \label{eq:bmdp}
                    \begin{array}{lcr}
                        \displaystyle \pi^* \in \argmax_{\pi\in\cM(\cA\times\textcolor{purple}{\cB})^{\cS\times\textcolor{purple}{\cB}}} \expectedvalue[G_r^\pi | s_0=s,\textcolor{purple}{\beta_0=\beta}] \\
                        \text{ s.t. }  \expectedvalue[G_c^\pi | s_0=s,\textcolor{purple}{\beta_0=\beta}] \leq \beta
                    \end{array}
                \end{equation}
            \end{itemize}
        \end{block}


        \begin{block}{}
            \begin{itemize}
                \item \cmoins \textit{Untractable}
                \item \cplus +1 DOF
                \item \cplus unfixed budget

                % A L ORAL On peut dÃ©finir un budget de safety et d'afranchir des lambda.}
            \end{itemize}
        \end{block}

    \end{frame}


    \begin{frame}{Augmented settings}

        \textbf{Budgeted Policies} $\pi\in\Pi$
        \begin{itemize}
            \item $ \pi:\underbrace{(s,\beta)}_{\os} \rightarrow \underbrace{(a,\beta')}_{\oa}$
        \end{itemize}

        \textbf{Domain}
        \begin{itemize}
            \item State $\ocS = \cS\times\cB$.
            \item Actions $\ocA = \cA\times\cB$.
            \item Dynamic $\ov{P}$
            %$\left((s',\beta') \condbar (s,\beta), (a, \beta_a)\right) \eqdef P(s'|s, a)\delta(\beta' - \beta_a)$.
        \end{itemize}
        \textbf{2D signals}
        \begin{itemize}
            \item Rewards $\ov{R} = (R, C)$
            \item Returns $G^\pi = (G_r^\pi, G_c^\pi)$
            \item $V^\pi(\os) = (V_r^\pi, V_c^\pi) \eqdef \expectedvalue\left[ G^\pi \condbar \ov{s_0} = \os\right]$
            \item $Q^\pi(\os, \oa)= (Q_r^\pi, Q_c^\pi) \eqdef \expectedvalue\left[ G^\pi \condbar \ov{s_0} = \os, \ov{a_0} = \oa\right]$
        \end{itemize}

    \end{frame}

    \begin{frame}{Optimality}
        \begin{definition}
            \begin{enumerate}
                \item[(i)] \pause\colorbox{red}{Respect the budget $\beta$}:
                \begin{equation*}
                    \Pi_a(\os) \eqdef \{\pi\in\Pi: V_c^\pi(s, \beta) \mathcolorbox{red}{\leq \beta}\}
                \end{equation*}
                \item[(ii)] \pause\colorbox{green}{Maximise rewards}:
                \begin{equation*}
                    V_r^*(\os) \eqdef \mathcolorbox{green}{\max}_{\pi\in\Pi_a(\os)}  V_r^\pi(\os) \qquad\quad \Pi_r(\os) \eqdef \mathcolorbox{green}{\argmax}_{\pi\in\Pi_a(\os)}  V_r^\pi(\os)
                \end{equation*}
                \item[(iii)] \pause\colorbox{yellow}{Minimise costs}:
                \begin{equation*}
                    V_c^*(\os) \eqdef \mathcolorbox{yellow}{\min}_{\pi\in\Pi_r(\os)}  V_c^\pi(\os), \qquad\quad \Pi^*(\os) \eqdef \mathcolorbox{yellow}{\argmin}_{\pi\in\Pi_r(\os)}  V_c^\pi(\os)
                \end{equation*}
            \end{enumerate}

            \pause We define $Q^*$ in the same fashion.
        \end{definition}
    \end{frame}

    \begin{frame}{Bellman optimality equation}
        \begin{block}{Bellman optimality equation}
            $Q^*$ verifies:
            \begin{align*}
                Q^{*}(\os, \oa) &= \cT Q^{*}(\os, \oa)\\
                &\eqdef \ov{R}(\os, \oa) + \gamma \sum_{\os'\in\ocS} \ov{P}(\ov{s'} | \os, \oa)\sum_{\ov{a'}\in \ocA} \pi_\text{greedy}(\ov{a'}|\ov{s'}; Q^*) Q^{*}(\ov{s'}, \ov{a'}),
            \end{align*}
            with:\pause
            \begin{align*}
                \pi_\text{greedy}(\cdot|\os; Q) \in &\mathcolorbox{yellow}{\argmin}_{\rho\in\Pi_r^Q} \sum_{\oa}\rho(\oa) Q_c(\os, \oa), \\
                \text{where }\quad \Pi_r^Q \eqdef &\mathcolorbox{green}{\argmax}_{\rho\in\cM(\ocA)} \sum_{\oa}\rho(\oa) Q_r(\os, \oa) \\
                & \text{ s.t. }   \sum_{\oa}\rho(\oa) Q_c(\os, \oa) \mathcolorbox{red}{\leq \beta}
            \end{align*}
        \end{block}
        % ORAL: comment rÃ©soudre ce problÃ¨me ?
    \end{frame}


    \foreach \n in {0,1,2,3,4}{
        \begin{frame}{Solving the non linear program}
            \begin{figure}
                \centering
                %%\vspace{-1.5em}
                \includegraphics[scale=1.0,page=1]{img/b\n}
            \end{figure}
        \end{frame}
    }

    \begin{frame}{Asymptotical behaviour}


        \begin{alertblock}{ \cmoins $\cT$ is not a contraction:}
            $\forall\epsilon>0, \exists Q^1,Q^2\in(\Real^2)^{\ocS\ocA}:\|\cT Q^1-\cT Q^2\|_\infty \geq \frac{1}{\epsilon}\|Q^1-Q^2\|_\infty$
        \end{alertblock}

        \begin{exampleblock}{\cplus unless $Q$-fonctions are smooth:}
            $\cT$ is a contraction for the subset $\cL_\gamma$ of $Q$-functions such that "$Q_r$ is $L$-Lipschitz w.r.t $Q_c$", with $L<\frac{1}{\gamma}-1$

        \end{exampleblock}
    \end{frame}

    \begin{frame}

        \begin{block}{Budgeted Value-Iteration}
            \begin{itemize}
                \item $Q \leftarrow \mathbf{0}$
                \item $Q(\os,\oa) \leftarrow \bo Q(\os,\oa)\ \forall (\os,\oa)$ jusqu'Ã  convergence % peut ne pas arriver
                \item Return $\pi(\os) = \pi_\text{greedy}(\cdot|\os; Q)$
            \end{itemize}
        \end{block}
        \pause
        \begin{alertblock}{}
            \begin{itemize}
                \item How to compute $\bo Q$ if $\transition$, $\reward$ et $\constraint$ are unknown?
                \begin{itemize}
                    \item Sampling with $\mathcal{D}=\{(\os_i,\oa_i,\ov{r}_i',\os_i')\}_{i\in[0,N]}$
                \end{itemize}
                \item How to compute $Q\ \forall (\os,\oa) \in \ocS\times\ocA$ if $\ocS$ is large or continuous ?
                \begin{itemize}
                    \item Function approximation
                \end{itemize}
            \end{itemize}

        \end{alertblock}
        \pause
        \begin{block}{Budgeted Fitted-Q}
            \begin{itemize}
                \item $Q \leftarrow \mathbf{0}$
                \item $Q \leftarrow \Gamma(\{\os_{\indextransition},\oa_{\indextransition}\}_{{\indextransition}\in \T},\{\ov{r}'_{\indextransition} + \gamma \sum_{\ov{a}'\in \cA} \pi_\text{greedy}(\ov{a}'|\ov{s}'_{\indextransition}; Q) Q(\ov{s}'_{\indextransition}, \ov{a}')_{{\indextransition} \in \T}\})$
                \item Return $\pi(\os) = \pi_\text{greedy}(\cdot|\os; Q)$
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}{Experiments: dialogue systems}
        \begin{center}
            \includegraphics[scale=0.9]{img/slot-filling.pdf}
        \end{center}
    \end{frame}

    \begin{frame}{Experiments: autonomous driving}
        \begin{center}
            \includegraphics[scale=0.9]{img/highway.pdf}
        \end{center}
    \end{frame}


\end{document}

